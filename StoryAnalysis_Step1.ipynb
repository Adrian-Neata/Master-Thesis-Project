{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxG8yamC3AXh",
        "outputId": "18125875-79a7-4c01-d6da-5139347fe8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.3/320.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install textacy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpej5dot8MaK",
        "outputId": "47616e42-475b-4313-eb3b-cd0f30aa3fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPl57M9aew0-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import ast\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textacy\n",
        "from textacy import text_stats\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "from langdetect import detect\n",
        "from langdetect import detect_langs\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt4vMzcmgu-m"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHFecVr3HGpG",
        "outputId": "be5c8833-cc01-4684-8f21-fc7f3e675afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TetnYXX8-KJl"
      },
      "outputs": [],
      "source": [
        "DEPENDENCY_TAGS = ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN36rTLqqPu5",
        "outputId": "50baa043-4e4f-4ac3-c36c-5daddc734bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-05 17:47:55.933940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxmAXXv0M4Zy"
      },
      "source": [
        "## Auxiliary Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNZDfUduM5_6"
      },
      "outputs": [],
      "source": [
        "def remove_short_texts(df, min_len, max_len):\n",
        "  df['text_length'] = df['text'].str.len()\n",
        "  df = df.drop(df.loc[df['text_length'] < min_len].index)\n",
        "  return df.drop(df.loc[df['text_length'] > max_len].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Preprocess Text Datasets"
      ],
      "metadata": {
        "id": "5xlk6-7sWXqx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VACTw33VxVKL"
      },
      "source": [
        "## Narations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4PVpLNpgyC9",
        "outputId": "a8caef58-6c31-47d4-ae45-ccd1f49ba484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 10000-books-and-their-genres-standardized.zip to /content\n",
            " 99% 1.29G/1.30G [00:16<00:00, 66.0MB/s]\n",
            "100% 1.30G/1.30G [00:16<00:00, 85.8MB/s]\n",
            "Archive:  10000-books-and-their-genres-standardized.zip\n",
            "  inflating: books_and_genres.csv    \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download michaelrussell4/10000-books-and-their-genres-standardized\n",
        "!unzip 10000-books-and-their-genres-standardized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txVktkvNhsAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2e3b47-b52c-4b69-8530-f2c7fccbb7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  del sys.path[0]\n"
          ]
        }
      ],
      "source": [
        "data_books = pd.read_csv('books_and_genres.csv')\n",
        "data_books.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "data_books.dropna(subset=['text'], inplace = True)\n",
        "data_books.reset_index(drop=True, inplace = True)\n",
        "\n",
        "for genre in ['poetry', 'religion', 'philosophy', 'historical', 'classics', 'humor', 'politics', 'short-stories', 'read-for-school', 'roman', 'non-fiction', 'history', '21st-century', 'literature', 'novels', 'fiction', 'horror', 'fantasy', 'speculative-fiction', 'science-fiction', 'supernatural', 'paranormal', '20th-century', 'dark', 'adult', 'mystery', 'adult-fiction', 'feminism', 'memoir', 'family', 'literary-fiction', 'college', 'high-school', 'american', 'mythology', 'biography', 'school', 'contemporary', 'historical-fiction', 'essays', 'coming-of-age', 'vampires', 'romance', 'novella', 'lgbt', 'thriller', 'unfinished', 'mystery-thriller', 'suspense', 'crime', 'science', 'theology', 'travel', 'adventure', 'relationships', 'cookbooks', 'reference', 'childrens', 'graphic-novels', 'comics', 'picture-books', 'action', 'magic', 'urban-fantasy', 'love', 'young-adult', 'paranormal-romance', 'teen', 'plays', 'dystopia', 'christian', 'education', 'psychology', 'economics', 'spirituality', 'writing', 'drama', 'art', 'middle-grade', 'christmas', 'chick-lit', 'new-adult', 'war', 'business', 'sociology', 'amazon', 'comedy', 'manga', 'erotica', 'animals', 'modern', 'self-help', 'realistic-fiction', 'historical-romance', 'sports', 'contemporary-romance', 'romantic-suspense', 'music', 'death', 'bdsm']:\n",
        "  data_books[genre] = 0\n",
        "\n",
        "for i in range(len(data_books)):\n",
        "  for genre in ast.literal_eval(data_books.iloc[i].genres):\n",
        "    data_books.at[i, genre] = 1\n",
        "\n",
        "data_books['lang'] = data_books.text.apply(detect)\n",
        "data_books.drop(data_books[data_books.lang != 'en'].index, inplace=True)\n",
        "data_books.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvysZhSTg5Qv"
      },
      "outputs": [],
      "source": [
        "data_books_list = [data_books.loc[i:i+106-1,:] for i in range(0, len(data_books), 106)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Short Stories"
      ],
      "metadata": {
        "id": "lPYhYrNcI7mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download ratthachat/writing-prompts\n",
        "!unzip writing-prompts.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz9WKdMkI8BA",
        "outputId": "c8e0e1f1-e5ae-4708-d69c-5bceab6a1616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading writing-prompts.zip to /content\n",
            " 94% 349M/370M [00:03<00:00, 94.2MB/s]\n",
            "100% 370M/370M [00:03<00:00, 118MB/s] \n",
            "Archive:  writing-prompts.zip\n",
            "  inflating: writingPrompts/README   \n",
            "  inflating: writingPrompts/test.wp_source  \n",
            "  inflating: writingPrompts/test.wp_target  \n",
            "  inflating: writingPrompts/train.wp_source  \n",
            "  inflating: writingPrompts/train.wp_target  \n",
            "  inflating: writingPrompts/valid.wp_source  \n",
            "  inflating: writingPrompts/valid.wp_target  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "short_story_dict = {\n",
        "    'prompt'  : [],\n",
        "    'text'   : [],\n",
        "}\n",
        "\n",
        "#aux_train_target = open('writingPrompts/train.wp_target', 'r').read().split('\\n')\n",
        "aux_test_target = open('writingPrompts/test.wp_target', 'r').read().split('\\n')\n",
        "aux_valid_target = open('writingPrompts/valid.wp_target', 'r').read().split('\\n')\n",
        "\n",
        "#aux_train_source = open('writingPrompts/train.wp_source', 'r').read().split('\\n')\n",
        "aux_test_source = open('writingPrompts/test.wp_source', 'r').read().split('\\n')\n",
        "aux_valid_source = open('writingPrompts/valid.wp_source', 'r').read().split('\\n')\n",
        "\n",
        "# for i in range(len(aux_train_target)):\n",
        "#   if aux_train_target[i] != '' and aux_train_source[i] != '':\n",
        "#     short_story_dict['prompt'].append(aux_train_source[i])\n",
        "#     short_story_dict['text'].append(aux_train_target[i].replace('<newline>', '\\n'))\n",
        "\n",
        "for i in range(len(aux_test_target)):\n",
        "  if aux_test_target[i] != '' and aux_test_source[i] != '':\n",
        "    short_story_dict['prompt'].append(aux_test_source[i])\n",
        "    short_story_dict['text'].append(aux_test_target[i].replace('<newline>', '\\n'))\n",
        "\n",
        "for i in range(len(aux_valid_target)):\n",
        "  if aux_valid_target[i] != '' and aux_valid_source[i] != '':\n",
        "    short_story_dict['prompt'].append(aux_valid_source[i])\n",
        "    short_story_dict['text'].append(aux_valid_target[i].replace('<newline>', '\\n'))\n",
        "\n",
        "data_shortstory = pd.DataFrame(short_story_dict)"
      ],
      "metadata": {
        "id": "yjuEk2wTJHmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_shortstory_list = [data_shortstory.loc[i:i+30336-1,:] for i in range(0, len(data_shortstory), 30336)]"
      ],
      "metadata": {
        "id": "JASyz729Bhm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## News"
      ],
      "metadata": {
        "id": "3mpLFu-X5eOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download snapcrack/all-the-news\n",
        "!unzip all-the-news.zip"
      ],
      "metadata": {
        "id": "jxXwpLOx5fbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_news = pd.concat([pd.read_csv('articles1.csv'), pd.read_csv('articles2.csv'), pd.read_csv('articles3.csv')], axis=0)\n",
        "data_news.rename(columns = {'content':'text'}, inplace = True)\n",
        "data_news.dropna(subset=['text'], inplace = True)\n",
        "data_news.drop(columns=['Unnamed: 0'], inplace = True)\n",
        "data_news = remove_short_texts(data_news, min_len=1500, max_len=10000)\n",
        "data_news.reset_index(drop=True, inplace = True)"
      ],
      "metadata": {
        "id": "hgl5UXRe5gR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJxHfsE8HmX"
      },
      "source": [
        "## Poems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJMPOrJZzVla",
        "outputId": "549d8c44-5a53-48d8-ad49-5348f03e67c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading poetry-foundation-poems.zip to /content\n",
            "100% 8.88M/8.88M [00:00<00:00, 18.6MB/s]\n",
            "100% 8.88M/8.88M [00:00<00:00, 18.7MB/s]\n",
            "Archive:  poetry-foundation-poems.zip\n",
            "  inflating: PoetryFoundationData.csv  \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download tgdivy/poetry-foundation-poems\n",
        "!unzip poetry-foundation-poems.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPLIlkJNUKZX"
      },
      "outputs": [],
      "source": [
        "data_poems = pd.read_csv('PoetryFoundationData.csv')\n",
        "data_poems.rename(columns = {'Poem':'text'}, inplace = True)\n",
        "data_poems.dropna(subset=['text'], inplace = True)\n",
        "data_poems = remove_short_texts(data_poems, 300, 2500)\n",
        "data_poems.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM4YITS70ivZ"
      },
      "source": [
        "## Recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agajn6cv0ke_",
        "outputId": "12acfead-804c-4061-9806-1ae579e4b102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading foodcom-recipes-with-search-terms-and-tags.zip to /content\n",
            " 98% 227M/231M [00:09<00:00, 22.2MB/s]\n",
            "100% 231M/231M [00:09<00:00, 26.2MB/s]\n",
            "Archive:  foodcom-recipes-with-search-terms-and-tags.zip\n",
            "  inflating: recipes_w_search_terms.csv  \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download shuyangli94/foodcom-recipes-with-search-terms-and-tags\n",
        "!unzip foodcom-recipes-with-search-terms-and-tags.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWCdIOhVbNwI"
      },
      "outputs": [],
      "source": [
        "data_recipes = pd.read_csv('recipes_w_search_terms.csv')\n",
        "data_recipes['steps'] = data_recipes['steps'].apply(lambda x: ' '.join(ast.literal_eval(x)))\n",
        "data_recipes.rename(columns = {'steps':'text'}, inplace = True)\n",
        "data_recipes.dropna(subset=['text'], inplace = True)\n",
        "data_recipes = remove_short_texts(data_recipes, min_len=200, max_len=2000)\n",
        "data_recipes.reset_index(drop=True, inplace = True)\n",
        "data_recipes = data_recipes.sample(n=100000, random_state=42)\n",
        "data_recipes.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTg7COsGrFQu"
      },
      "source": [
        "## Presentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Yq0I9KrG6N",
        "outputId": "5ad89b80-90dd-4aea-cb27-a5466e11c70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ted-talk.zip to /content\n",
            " 34% 9.00M/26.8M [00:00<00:00, 59.8MB/s]\n",
            "100% 26.8M/26.8M [00:00<00:00, 129MB/s] \n",
            "Archive:  ted-talk.zip\n",
            "  inflating: TED_Talk.csv            \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download thegupta/ted-talk\n",
        "!unzip ted-talk.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QxCVxx6Vq1s"
      },
      "outputs": [],
      "source": [
        "data_ted = pd.read_csv('TED_Talk.csv')\n",
        "data_ted.rename(columns = {'transcript':'text'}, inplace = True)\n",
        "data_ted.dropna(subset=['text'], inplace = True)\n",
        "data_ted['text'] = data_ted['text'].apply(lambda x: x.replace('(Audience)', ' ').replace('(Laughter)', ' ').replace('(Applause)', ' ').replace('(Music)', ' '))\n",
        "data_ted = remove_short_texts(data_ted, min_len=2000, max_len=20000)\n",
        "data_ted.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFOY6OrpuM5F"
      },
      "source": [
        "## Movie Dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybPx5wBIuPBs",
        "outputId": "bc498212-e03d-4aad-fc2e-3e100e9c6b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading movie-dialog-corpus.zip to /content\n",
            " 56% 5.00M/8.91M [00:00<00:00, 22.4MB/s]\n",
            "100% 8.91M/8.91M [00:00<00:00, 37.4MB/s]\n",
            "Archive:  movie-dialog-corpus.zip\n",
            "  inflating: README.txt              \n",
            "  inflating: movie_characters_metadata.tsv  \n",
            "  inflating: movie_conversations.tsv  \n",
            "  inflating: movie_lines.tsv         \n",
            "  inflating: movie_titles_metadata.tsv  \n",
            "  inflating: raw_script_urls.tsv     \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download Cornell-University/movie-dialog-corpus\n",
        "!unzip movie-dialog-corpus.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FPzh4Wioux6"
      },
      "outputs": [],
      "source": [
        "def get_text_conversation(x):\n",
        "\n",
        "  # convert string to list of conversation ids\n",
        "  x = x[1:-1].split(' ')\n",
        "\n",
        "  # remove \\' before and after each line id\n",
        "  x = [x[i][1:-1] for i in range(len(x))]\n",
        "\n",
        "  # replace line id with text\n",
        "  aux_x = []\n",
        "  for line_id in x:\n",
        "\n",
        "    if line_id not in aux_data_lines.index:\n",
        "      aux_x = []\n",
        "      break\n",
        "\n",
        "    aux_x.append(aux_data_lines.loc[line_id].text)\n",
        "\n",
        "  return '\\n'.join(aux_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8DYN4UGlXcc"
      },
      "outputs": [],
      "source": [
        "aux_data_lines = pd.read_csv(\n",
        "    'movie_lines.tsv',\n",
        "    encoding='utf-8-sig',\n",
        "    sep='\\t',\n",
        "    header = None,\n",
        "    names = ['lineID', 'charID', 'movieID', 'charName', 'text'],\n",
        "    index_col=['lineID']\n",
        ")\n",
        "aux_data_lines.dropna(subset=['text'], inplace = True)\n",
        "\n",
        "data_conversations = pd.read_csv(\n",
        "    'movie_conversations.tsv',\n",
        "    sep='\\t',\n",
        "    encoding='ISO-8859-2',\n",
        "    names = ['charID_1', 'charID_2', 'movieID', 'conversation']\n",
        ")\n",
        "data_conversations.dropna(subset=['conversation'], inplace = True)\n",
        "\n",
        "# convert line ids to conversation\n",
        "data_conversations['conversation'] = data_conversations['conversation'].apply(lambda x: get_text_conversation(x))\n",
        "\n",
        "data_conversations.rename(columns = {'conversation':'text'}, inplace = True)\n",
        "data_conversations = remove_short_texts(data_conversations, min_len=300, max_len=1500)\n",
        "data_conversations.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIHe1qkNvGF3"
      },
      "source": [
        "## Wine Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IixAkZmlvIMr",
        "outputId": "137e8cc4-62f9-4f5f-be45-d20a2a362263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading wine-reviews.zip to /content\n",
            " 92% 47.0M/50.9M [00:00<00:00, 260MB/s]\n",
            "100% 50.9M/50.9M [00:00<00:00, 247MB/s]\n",
            "Archive:  wine-reviews.zip\n",
            "  inflating: winemag-data-130k-v2.csv  \n",
            "  inflating: winemag-data-130k-v2.json  \n",
            "  inflating: winemag-data_first150k.csv  \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download zynicide/wine-reviews\n",
        "!unzip wine-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOs9ViQ3vStj"
      },
      "outputs": [],
      "source": [
        "data_wine = pd.read_csv('winemag-data_first150k.csv')\n",
        "data_wine.rename(columns = {'description':'text'}, inplace = True)\n",
        "data_wine.dropna(subset=['text'], inplace = True)\n",
        "data_wine.drop(columns=['Unnamed: 0'], inplace = True)\n",
        "data_wine = remove_short_texts(data_wine, min_len=200, max_len=500)\n",
        "data_wine.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dT5x4qcQIGw"
      },
      "source": [
        "## Religious Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icQOBcp4QKPN",
        "outputId": "4cf6470d-556b-4855-cc16-1886e448da05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading religious-and-philosophical-texts.zip to /content\n",
            "\r  0% 0.00/2.57M [00:00<?, ?B/s]\n",
            "\r100% 2.57M/2.57M [00:00<00:00, 120MB/s]\n",
            "Archive:  religious-and-philosophical-texts.zip\n",
            "  inflating: 35895-0.txt             \n",
            "  inflating: pg10.txt                \n",
            "  inflating: pg17.txt                \n",
            "  inflating: pg2680.txt              \n",
            "  inflating: pg2800.txt              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download tentotheminus9/religious-and-philosophical-texts\n",
        "!unzip religious-and-philosophical-texts.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgJHzMIOAaP8"
      },
      "outputs": [],
      "source": [
        "religion_dict = {\n",
        "    'author'  : [],\n",
        "    'title'   : [],\n",
        "    'text'    : []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOPnRpsih8_N"
      },
      "outputs": [],
      "source": [
        "james_bible = open('pg10.txt', 'r').read()\n",
        "start_idx = james_bible.find('*** START OF THIS PROJECT GUTENBERG EBOOK THE KING JAMES BIBLE ***\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n') + len('*** START OF THIS PROJECT GUTENBERG EBOOK THE KING JAMES BIBLE ***\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
        "final_idx = james_bible.find('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnd of the Project Gutenberg EBook of The King James Bible')\n",
        "james_bible = james_bible[start_idx:final_idx]\n",
        "\n",
        "works = ['The First Book of Moses:  Called Genesis', 'The Second Book of Moses:  Called Exodus', 'The Third Book of Moses:  Called Leviticus', 'The Fourth Book of Moses:  Called Numbers', 'The Fifth Book of Moses:  Called Deuteronomy', 'The Book of Joshua', 'The Book of Judges']\n",
        "fillers = []\n",
        "for i in range(50, 0, -1):\n",
        "  for j in range(50, 0, -1):\n",
        "    fillers.append(str(i) + ':' + str(j) + ' ')\n",
        "\n",
        "b = []\n",
        "for i in range(len(works)):\n",
        "  if i < len(works) - 1:\n",
        "    start = james_bible.find(works[i])\n",
        "    end = james_bible.find(works[i+1])\n",
        "    b.append(james_bible[start:end])\n",
        "  else:\n",
        "    start = james_bible.find(works[i])\n",
        "    b.append(james_bible[start:])\n",
        "\n",
        "for i in range(len(b)):\n",
        "  for f in fillers:\n",
        "    b[i] = b[i].replace(f, '')\n",
        "  religion_dict['author'].append(None)\n",
        "  religion_dict['title'].append(works[i])\n",
        "  religion_dict['text'].append(b[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOApeibR9aAI"
      },
      "outputs": [],
      "source": [
        "mormon_book = open('pg17.txt', 'r').read()\n",
        "start_idx = mormon_book.find('*** START OF THIS PROJECT GUTENBERG EBOOK THE BOOK OF MORMON ***\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n') + len('*** START OF THIS PROJECT GUTENBERG EBOOK THE BOOK OF MORMON ***\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
        "final_idx = mormon_book.find('\\n\\n\\n\\nEnd of the Project Gutenberg EBook of The Book Of Mormon')\n",
        "mormon_book = mormon_book[start_idx:final_idx]\n",
        "\n",
        "works = ['THE TESTIMONY OF THREE WITNESSES', 'THE TESTIMONY OF EIGHT WITNESSES', 'THE FIRST BOOK OF NEPHI HIS REIGN AND MINISTRY (1 Nephi)', 'THE SECOND BOOK OF NEPHI2', 'THE BOOK OF JACOB', 'THE BOOK OF MOSIAH', 'THE BOOK OF ALMA', 'THE BOOK OF HELAMAN', 'THIRD BOOK OF NEPHI', 'THE BOOK OF MORMON', 'THE BOOK OF ETHER', 'THE BOOK OF MORONI']\n",
        "\n",
        "b = []\n",
        "for i in range(len(works)):\n",
        "  if i < len(works) - 1:\n",
        "    start = mormon_book[10:].find(works[i])\n",
        "    end = mormon_book[10:].find(works[i+1])\n",
        "    b.append(mormon_book[start:end])\n",
        "  else:\n",
        "    start = mormon_book[10:].find(works[i])\n",
        "    b.append(mormon_book[start:])\n",
        "\n",
        "for i in range(len(b)):\n",
        "  b[i] = list(filter(lambda x: len(x) >= 15, b[i].split('\\n')))\n",
        "  for j in range(len(b[i])):\n",
        "    for k in range(len(b[i][j])):\n",
        "      if b[i][j][k].isalpha():\n",
        "        break\n",
        "    b[i][j] = b[i][j][k:]\n",
        "  b[i] = '\\n'.join(b[i])\n",
        "  religion_dict['author'].append(None)\n",
        "  religion_dict['title'].append(works[i])\n",
        "  religion_dict['text'].append(b[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpc5pE5TAybF"
      },
      "outputs": [],
      "source": [
        "buddha_gospel = open('35895-0.txt', 'r').read()\n",
        "start_idx = buddha_gospel.find('[Illustration]\\n\\n\\n\\n\\n') + len('[Illustration]\\n\\n\\n\\n\\n')\n",
        "final_idx = buddha_gospel.find('[Names and terms must be looked up in the Glossary')\n",
        "buddha_gospel = buddha_gospel[start_idx:final_idx]\n",
        "\n",
        "religion_dict['author'].append(None)\n",
        "religion_dict['title'].append('THE GOSPEL OF BUDDHA')\n",
        "religion_dict['text'].append(buddha_gospel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7jCLaPTDMIn"
      },
      "outputs": [],
      "source": [
        "marcus_meditations = open('pg2680.txt', 'r').read()\n",
        "start_idx = marcus_meditations.find('thou have them always in a readiness.\\n\\n\\n') + len('thou have them always in a readiness.\\n\\n\\n')\n",
        "final_idx = marcus_meditations.find('\\n\\n\\n\\nAPPENDIX')\n",
        "marcus_meditations = marcus_meditations[start_idx:final_idx]\n",
        "\n",
        "works = ['THE FIRST BOOK', 'THE SECOND BOOK', 'THE THIRD BOOK', 'THE FOURTH BOOK', 'THE FIFTH BOOK', 'THE SIXTH BOOK', 'THE SEVENTH BOOK', 'THE EIGHTH BOOK', 'THE NINTH BOOK', 'THE TENTH BOOK', 'THE ELEVENTH BOOK', 'THE TWELFTH BOOK']\n",
        "\n",
        "b = []\n",
        "for i in range(len(works)):\n",
        "  if i < len(works) - 1:\n",
        "    start = marcus_meditations.find(works[i])\n",
        "    end = marcus_meditations.find(works[i+1])\n",
        "    b.append(marcus_meditations[start:end])\n",
        "  else:\n",
        "    start = marcus_meditations.find(works[i])\n",
        "    b.append(marcus_meditations[start:])\n",
        "\n",
        "for i in range(len(b)):\n",
        "  religion_dict['author'].append('Marcus Aurelius')\n",
        "  religion_dict['title'].append(works[i])\n",
        "  religion_dict['text'].append(b[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4anGrkjdEPYN"
      },
      "outputs": [],
      "source": [
        "# too hard to decipher :(\n",
        "# koran = open('pg2800.txt', 'r').read()\n",
        "# start_idx = koran.find('SURA1 XCVI.-THICK BLOOD, OR CLOTS OF BLOOD [I.]')\n",
        "# final_idx = koran.find('\\n\\n\\n\\n\\n\\nEnd of The Project Gutenberg Etext of The Koran as translated by')\n",
        "# koran = koran[start_idx:final_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMI4A99p95K1"
      },
      "outputs": [],
      "source": [
        "data_religion = pd.DataFrame(religion_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a75DUr52QY1"
      },
      "source": [
        "# Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJhfDFOI2SMB"
      },
      "outputs": [],
      "source": [
        "class TextFeaturizer():\n",
        "\n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "    if len(text) > 1000000:\n",
        "      self.doc = []\n",
        "      for length in range(300000, len(text), 300000):\n",
        "        self.doc.append(textacy.make_spacy_doc(text[length-300000:length], lang='en_core_web_lg'))\n",
        "      self.doc.append(textacy.make_spacy_doc(text[length:], lang='en_core_web_lg'))\n",
        "    else:\n",
        "      self.doc = [textacy.make_spacy_doc(text, lang='en_core_web_lg')]\n",
        "    self.n_sents =  [text_stats.n_sents(d) for d in self.doc]\n",
        "    self.n_words =  [text_stats.n_words(d) for d in self.doc]\n",
        "\n",
        "  def questions_frequency(self):\n",
        "    n_sents = 0\n",
        "    for ts in self.n_sents:\n",
        "      n_sents += ts\n",
        "    return self.text.count('?') / n_sents\n",
        "\n",
        "  def words_per_sentence(self):\n",
        "    n_words = 0\n",
        "    n_sents = 0\n",
        "    for i in range(len(self.doc)):\n",
        "      n_words += self.n_words[i]\n",
        "      n_sents += self.n_sents[i]\n",
        "    return n_words / n_sents\n",
        "\n",
        "  def ttr(self):\n",
        "    \"\"\"type/token ratio (TTR)\"\"\"\n",
        "\n",
        "    stop_pos = ['ADP', 'CONJ', 'CCONJ', 'DET', 'EOL', 'PART', 'PUNCT', 'SPACE', 'SCONJ', 'SYM', 'X']\n",
        "    token_count = 0\n",
        "    types = set()\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for token in doc:\n",
        "        if not token.pos_ in stop_pos:\n",
        "          types.add(token.lemma)\n",
        "          token_count += 1\n",
        "\n",
        "      if token_count == 0:\n",
        "        return 0\n",
        "    return len(types) / token_count\n",
        "\n",
        "  def sttr(self, n=1000):\n",
        "    \"\"\"standardized type/token ratio (STTR) for every n words\"\"\"\n",
        "\n",
        "    stop_pos = ['ADP', 'CONJ', 'CCONJ', 'DET', 'EOL', 'PART', 'PUNCT', 'SPACE', 'SCONJ', 'SYM', 'X']\n",
        "    token_count = 0\n",
        "    types = set()\n",
        "    ttr = []\n",
        "    words_read = 0\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for token in doc:\n",
        "        if words_read >= n:\n",
        "          words_read = 0\n",
        "          if token_count != 0:\n",
        "            ttr.append(len(types) / token_count)\n",
        "          types.clear()\n",
        "          token_count = 0\n",
        "        if not token.pos_ in stop_pos:\n",
        "          types.add(token.lemma)\n",
        "          token_count += 1\n",
        "        words_read += 1\n",
        "\n",
        "    if (words_read > n / 4 or ttr == []) and token_count != 0:\n",
        "      ttr.append(len(types) / token_count)\n",
        "\n",
        "    return sum(ttr) / len(ttr)\n",
        "\n",
        "  def pos_freq(self):\n",
        "    \"\"\" frequency of parts of speech\n",
        "    \"\"\"\n",
        "    distribution = {}\n",
        "    for pos in ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']:\n",
        "      distribution[pos] = 0\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for token in doc:\n",
        "        distribution[token.pos_] += 1\n",
        "\n",
        "    total_count = 0\n",
        "    for pos in distribution:\n",
        "      total_count += distribution[pos]\n",
        "\n",
        "    if total_count == 0:\n",
        "      return distribution\n",
        "\n",
        "    for i in distribution:\n",
        "      distribution[i] /= total_count\n",
        "\n",
        "    return distribution\n",
        "\n",
        "  def dep_freq(self):\n",
        "    \"\"\" frequency of dependencies\n",
        "    \"\"\"\n",
        "    distribution = {}\n",
        "    for dep in DEPENDENCY_TAGS:\n",
        "      distribution[dep] = 0\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for token in doc:\n",
        "        distribution[token.dep_] += 1\n",
        "\n",
        "    total_count = 0\n",
        "    for dep in distribution:\n",
        "      total_count += distribution[dep]\n",
        "\n",
        "    if total_count == 0:\n",
        "      return distribution\n",
        "\n",
        "    for i in distribution:\n",
        "      distribution[i] /= total_count\n",
        "\n",
        "    return distribution\n",
        "\n",
        "  def deixis_freq(self):\n",
        "    \"\"\"frequency of deixis\n",
        "    \"\"\"\n",
        "\n",
        "    stop_pos = ['ADP', 'CONJ', 'CCONJ', 'EOL', 'PUNCT', 'SPACE', 'SCONJ', 'SYM', 'X']\n",
        "    deixis_count = 0\n",
        "    token_count = 0\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for token in doc:\n",
        "        if token.pos_ not in stop_pos:\n",
        "          token_count += 1\n",
        "          if token.pos_ == \"DET\" and token.text.lower() not in ['the', 'a', 'an', 'no']:\n",
        "            deixis_count += 1\n",
        "          if token.pos_ == \"PRON\":\n",
        "            deixis_count += 1\n",
        "          if token.pos_ == \"ADV\" and token.text.lower() in [\"here\",\"there\",\"now\",\"then\",\"soon\",\"today\",\"tonight\",\"yesterday\",\"tomorrow\",\"above\",\"below\",\"behind\",\"ahead\",\"nearby\",\"far\",\"away\",\"inside\",\"outside\",\"somewhere\",\"everywhere\",\"anywhere\",]:\n",
        "            deixis_count += 1\n",
        "\n",
        "    if token_count == 0:\n",
        "      return 0.0\n",
        "\n",
        "    return deixis_count / token_count\n",
        "\n",
        "  def verb_tense_frequency(self):\n",
        "    \"\"\"distribution of verb tenses\n",
        "    \"\"\"\n",
        "\n",
        "    distribution = { 'Past': 0, 'Present': 0, 'Future': 0}\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for i in range(len(doc)):\n",
        "        token = doc[i]\n",
        "\n",
        "        # check for cases where auxiliary verb 'will' or 'shall' is used for future\n",
        "        if token.pos_ in ['AUX', 'MD']:\n",
        "\n",
        "          if token.lemma_.lower() in ['will', 'shall']:\n",
        "            distribution['Future'] += 1\n",
        "\n",
        "        if token.pos_ == 'VERB':\n",
        "\n",
        "          if token.tag_ in ['VBD', 'VBN']:\n",
        "            distribution['Past'] += 1\n",
        "\n",
        "          elif token.tag_ in ['VBP', 'VBZ', 'VBG']:\n",
        "            distribution['Present'] += 1\n",
        "\n",
        "          elif token.tag_ in ['VB'] and i > 1:\n",
        "\n",
        "            # check for 'going to' future\n",
        "            if doc[i-1].text.lower() == 'to' and doc[i-2].text.lower() == 'going':\n",
        "              distribution['Future'] += 1\n",
        "\n",
        "              # false alarm, gets triggered every 'going to' scenario\n",
        "              distribution['Present'] -= 1\n",
        "\n",
        "    total_count = distribution['Past'] + distribution['Present'] + distribution['Future']\n",
        "    if total_count == 0:\n",
        "      return distribution\n",
        "\n",
        "    for i in distribution:\n",
        "      distribution[i] /= total_count\n",
        "\n",
        "    return distribution\n",
        "\n",
        "  def person_frequency(self):\n",
        "    \"\"\"frequency of pronoun persons\n",
        "    \"\"\"\n",
        "    distribution = {\n",
        "        1: 0,\n",
        "        2: 0,\n",
        "        3: 0\n",
        "    }\n",
        "\n",
        "    for doc in self.doc:\n",
        "      for token in doc:\n",
        "        if token.pos_ == \"PRON\":\n",
        "\n",
        "          if token.text.lower() in ['i', 'we', 'me', 'my', 'mine', 'myself', 'us', 'our', 'ours', 'ourselves']:\n",
        "            distribution[1] += 1\n",
        "\n",
        "          if token.text.lower() in ['you', 'your', 'yours', 'yourself']:\n",
        "            distribution[2] += 1\n",
        "\n",
        "          if token.text.lower() in ['he', 'she', 'they', 'him', 'himself', 'her', 'herself', 'them', 'his', 'hers', 'their']:\n",
        "            distribution[3] += 1\n",
        "\n",
        "    total_count = distribution[1] + distribution[2] + distribution[3]\n",
        "    if total_count == 0:\n",
        "      return distribution\n",
        "\n",
        "    for i in distribution:\n",
        "      distribution[i] /= total_count\n",
        "\n",
        "    return distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0pNkNDr3uQD"
      },
      "outputs": [],
      "source": [
        "def extract_features(dataset, idx=0):\n",
        "  # initialize new columns for features\n",
        "  text_features = ['question_frequency', 'words_per_sentence', 'ttr', 'sttr', 'deixis_frequency', 'tense_present_frequency', 'tense_past_frequency',\n",
        "                   'tense_future_frequency', 'person_1_frequency', 'person_2_frequency', 'person_3_frequency', 'ADJ', 'ADP', 'ADV', 'AUX', 'CONJ',\n",
        "                   'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE'] + DEPENDENCY_TAGS\n",
        "\n",
        "  for feature in text_features:\n",
        "    dataset[feature] = 0\n",
        "\n",
        "  for i in tqdm(range(len(dataset))):\n",
        "\n",
        "    featurizer = TextFeaturizer(dataset.iloc[i]['text'])\n",
        "    dataset.loc[idx+i,'question_frequency'] = featurizer.questions_frequency()\n",
        "    dataset.loc[idx+i,'words_per_sentence'] = featurizer.words_per_sentence()\n",
        "    dataset.loc[idx+i,'ttr'] = featurizer.ttr()\n",
        "    dataset.loc[idx+i,'sttr'] = featurizer.sttr(n=100)\n",
        "    dataset.loc[idx+i,'deixis_frequency'] = featurizer.deixis_freq()\n",
        "\n",
        "    tense_freq = featurizer.verb_tense_frequency()\n",
        "    dataset.loc[idx+i,'tense_present_frequency'] = tense_freq['Present']\n",
        "    dataset.loc[idx+i,'tense_past_frequency'] = tense_freq['Past']\n",
        "    dataset.loc[idx+i,'tense_future_frequency'] = tense_freq['Future']\n",
        "\n",
        "    person_freq = featurizer.person_frequency()\n",
        "    dataset.loc[idx+i,'person_1_frequency'] = person_freq[1]\n",
        "    dataset.loc[idx+i,'person_2_frequency'] = person_freq[2]\n",
        "    dataset.loc[idx+i,'person_3_frequency'] = person_freq[3]\n",
        "\n",
        "    pos_freq = featurizer.pos_freq()\n",
        "    for pos in pos_freq:\n",
        "      dataset.loc[idx+i,pos] = pos_freq[pos]\n",
        "\n",
        "    dep_freq = featurizer.dep_freq()\n",
        "    for dep in dep_freq:\n",
        "      dataset.loc[idx+i, dep] = dep_freq[dep]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features for downloaded dataset"
      ],
      "metadata": {
        "id": "5sjESA0RWPoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qVn4r9v108T"
      },
      "outputs": [],
      "source": [
        "idx = 0\n",
        "for i in range(0):\n",
        "  idx += len(data_books_list[i])\n",
        "\n",
        "for i in range(0, len(data_books_list)):\n",
        "  extract_features(data_books_list[i], idx)\n",
        "  data_books_list[i].to_csv('data_books' + str(i) + '.csv')\n",
        "  files.download('data_books' + str(i) + '.csv')\n",
        "  idx += len(data_books_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "books = []\n",
        "for i in range(86):\n",
        "  books.append(pd.read_csv('data_books' + str(i) + '.csv'))\n",
        "  books[-1].drop(columns=['Unnamed: 0', 'text', 'genres', 'poetry', 'religion', 'philosophy', 'historical', 'classics', 'humor', 'politics', 'short-stories', 'read-for-school', 'roman', 'non-fiction', 'history', '21st-century', 'literature', 'novels', 'fiction', 'horror', 'fantasy', 'speculative-fiction', 'science-fiction', 'supernatural', 'paranormal', '20th-century', 'dark', 'adult', 'mystery', 'adult-fiction', 'feminism', 'memoir', 'family', 'literary-fiction', 'college', 'high-school', 'american', 'mythology', 'biography', 'school', 'contemporary', 'historical-fiction', 'essays', 'coming-of-age', 'vampires', 'romance', 'novella', 'lgbt', 'thriller', 'unfinished', 'mystery-thriller', 'suspense', 'crime', 'science', 'theology', 'travel', 'adventure', 'relationships', 'cookbooks', 'reference', 'childrens', 'graphic-novels', 'comics', 'picture-books', 'action', 'magic', 'urban-fantasy', 'love', 'young-adult', 'paranormal-romance', 'teen', 'plays', 'dystopia', 'christian', 'education', 'psychology', 'economics', 'spirituality', 'writing', 'drama', 'art', 'middle-grade', 'christmas', 'chick-lit', 'new-adult', 'war', 'business', 'sociology', 'amazon', 'comedy', 'manga', 'erotica', 'animals', 'modern', 'self-help', 'realistic-fiction', 'historical-romance', 'sports', 'contemporary-romance', 'romantic-suspense', 'music', 'death', 'bdsm', 'lang'], inplace = True)\n",
        "data_books = pd.concat(books, axis=0)\n",
        "data_books.reset_index(drop=True, inplace=True)\n",
        "data_books.drop(2014, inplace=True)\n",
        "data_books.reset_index(drop=True, inplace=True)\n",
        "data_books.to_csv('data_books.csv')"
      ],
      "metadata": {
        "id": "CtJSp65I7OSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "for i in range(0):\n",
        "  idx += len(data_shortstory_list[i])\n",
        "\n",
        "for i in range(0, len(data_shortstory_list)):\n",
        "  extract_features(data_shortstory_list[i], idx)\n",
        "  #data_shortstory_list[i].to_csv('data_shortstory' + str(i) + '.csv')\n",
        "  #files.download('data_shortstory' + str(i) + '.csv')\n",
        "  idx += len(data_shortstory_list[i])"
      ],
      "metadata": {
        "id": "TNWpmXETN3f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shorts = []\n",
        "for i in range(10):\n",
        "  shorts.append(pd.read_csv('data_shortstory' + str(i) + '.csv'))\n",
        "  shorts[-1].drop(columns=['Unnamed: 0', 'text'], inplace = True)\n",
        "data_shorts = pd.concat(shorts, axis=0)\n",
        "data_shorts.reset_index(drop=True, inplace=True)\n",
        "data_shorts.to_csv('data_shortstory.csv')"
      ],
      "metadata": {
        "id": "jQV4QrNB5E1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svbX9hWgBUiO"
      },
      "outputs": [],
      "source": [
        "extract_features(data_news)\n",
        "data_news.to_csv('data_news.csv')\n",
        "files.download('data_news.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy1MSr1m9ub1"
      },
      "outputs": [],
      "source": [
        "extract_features(data_poems)\n",
        "data_poems.to_csv('data_poems.csv')\n",
        "files.download('data_poems.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e6qqyQk9W10"
      },
      "outputs": [],
      "source": [
        "extract_features(data_recipes)\n",
        "data_recipes.to_csv('data_recipes.csv')\n",
        "files.download('data_recipes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCqLIz2K-8j_"
      },
      "outputs": [],
      "source": [
        "extract_features(data_ted)\n",
        "data_ted.to_csv('data_ted.csv')\n",
        "files.download('data_ted.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJe2cN8T4CJk"
      },
      "outputs": [],
      "source": [
        "extract_features(data_conversations)\n",
        "data_conversations.to_csv('data_conversations.csv')\n",
        "files.download('data_conversations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF1m07QsLnQS"
      },
      "outputs": [],
      "source": [
        "extract_features(data_wine)\n",
        "data_wine.to_csv('data_wine.csv')\n",
        "files.download('data_wine.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoUu-Qpa3_73"
      },
      "outputs": [],
      "source": [
        "extract_features(data_religion)\n",
        "data_religion.to_csv('data_religion.csv')\n",
        "files.download('data_religion.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"chatgpt_datasets_output.txt\", \"r\")\n",
        "chatgpt_dataset_dict = {'text': []}\n",
        "\n",
        "line = f.readline()\n",
        "while line:\n",
        "  chatgpt_dataset_dict['text'].append(line)\n",
        "  line = f.readline()\n",
        "\n",
        "chatgpt_dataset = pd.DataFrame(chatgpt_dataset_dict)\n",
        "extract_features(chatgpt_dataset)\n",
        "chatgpt_dataset.to_csv('data_3story.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MziUNMyBM7F4",
        "outputId": "88172d8f-a279-445b-ecf1-ea671cae0fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2579/2579 [03:36<00:00, 11.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"comb_datasets_output.txt\", \"r\")\n",
        "comb_dataset_dict = {'text': []}\n",
        "\n",
        "line = f.readline()\n",
        "while line:\n",
        "  comb_dataset_dict['text'].append(line)\n",
        "  line = f.readline()\n",
        "\n",
        "comb_dataset = pd.DataFrame(comb_dataset_dict)\n",
        "extract_features(comb_dataset)\n",
        "comb_dataset.to_csv('data_3story_wp.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB9F-HeTN1im",
        "outputId": "f9bbe4a2-bb44-4611-aee9-4306ea8e8660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:08<00:00, 12.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"uniquechatgpt_datasets_output.txt\", \"r\")\n",
        "unique_chatgpt_dataset_dict = {'text': []}\n",
        "\n",
        "line = f.readline()\n",
        "while line:\n",
        "  unique_chatgpt_dataset_dict['text'].append(line)\n",
        "  line = f.readline()\n",
        "\n",
        "unique_chatgpt_dataset = pd.DataFrame(unique_chatgpt_dataset_dict)\n",
        "extract_features(unique_chatgpt_dataset)\n",
        "unique_chatgpt_dataset.to_csv('data_1story.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrTTPaw-OKTd",
        "outputId": "86a35d48-8861-4df7-f3af-b589d11477d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 12.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"writingprompts_datasets_output.txt\", \"r\")\n",
        "wp_dataset_dict = {'text': []}\n",
        "\n",
        "line = f.readline()\n",
        "while line:\n",
        "  wp_dataset_dict['text'].append(line)\n",
        "  line = f.readline()\n",
        "\n",
        "wp_dataset = pd.DataFrame(wp_dataset_dict)\n",
        "extract_features(wp_dataset)\n",
        "wp_dataset.to_csv('data_wp.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqj9OjUQOiEM",
        "outputId": "20e03c2d-940a-47b7-a9e7-c1b36ad38e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 12.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_dataset['method'] = 'ChatGPT3Stories'\n",
        "comb_dataset['method'] =  'ChatGPT3Stories + WritingPrompts'\n",
        "unique_chatgpt_dataset['method'] = 'ChatGPT1Story'\n",
        "wp_dataset['method'] = 'WritingPrompts'"
      ],
      "metadata": {
        "id": "YlIw1h4dPA_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols = ['ChatGPT3Stories', 'ChatGPT3Stories + WritingPrompts', 'ChatGPT1Story', 'WritingPrompts']\n",
        "feature_cols = ['question_frequency', 'words_per_sentence', 'sttr', 'deixis_frequency', 'tense_present_frequency', 'tense_past_frequency', 'tense_future_frequency', 'person_1_frequency', 'person_2_frequency', 'person_3_frequency', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE', 'ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
        "pos_cols = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB']\n",
        "dep_cols = ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
        "\n",
        "final_dataset = pd.concat([wp_dataset, unique_chatgpt_dataset, chatgpt_dataset, comb_dataset]).reset_index(drop=True)\n",
        "stat_dict = {'dataset' : []}\n",
        "for feat_col in feature_cols:\n",
        "  stat_dict[feat_col + '_mean'] = []\n",
        "  stat_dict[feat_col + '_median'] = []\n",
        "  stat_dict[feat_col + '_std'] = []\n",
        "\n",
        "for col in data_cols:\n",
        "  stat_dict['dataset'].append(col)\n",
        "  for feat_col in feature_cols:\n",
        "    stat_dict[feat_col + '_mean'].append(final_dataset.loc[final_dataset['method'] == col][feat_col].mean())\n",
        "    stat_dict[feat_col + '_median'].append(final_dataset.loc[final_dataset['method'] == col][feat_col].median())\n",
        "    stat_dict[feat_col + '_std'].append(final_dataset.loc[final_dataset['method'] == col][feat_col].std())"
      ],
      "metadata": {
        "id": "7RZ5cXd4jP6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols = ['ChatGPT3Stories', 'ChatGPT3Stories + WritingPrompts', 'ChatGPT1Story', 'WritingPrompts']\n",
        "feature_cols = ['question_frequency', 'words_per_sentence', 'sttr', 'deixis_frequency', 'tense_present_frequency', 'tense_past_frequency', 'tense_future_frequency', 'person_1_frequency', 'person_2_frequency', 'person_3_frequency', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE', 'ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
        "pos_cols = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB']\n",
        "dep_cols = ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
        "\n",
        "stat_dict = {'dataset' : []}\n",
        "for feat_col in feature_cols:\n",
        "  stat_dict[feat_col + '_mean'] = []\n",
        "  stat_dict[feat_col + '_median'] = []\n",
        "  stat_dict[feat_col + '_std'] = []\n",
        "for feat_col in feature_cols:\n",
        "    stat_dict[feat_col + '_mean'].append(chatgpt_dataset[feat_col].mean())\n",
        "    stat_dict[feat_col + '_median'].append(chatgpt_dataset[feat_col].median())\n",
        "    stat_dict[feat_col + '_std'].append(chatgpt_dataset[feat_col].std())\n"
      ],
      "metadata": {
        "id": "ke4nm8DKMS-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset = {'text': [], 'method':[]}\n",
        "for filename in ['GPT2+ChtGPT.txt', 'GPT2-WP+ChatGPT.txt', 'last_hope_GAN_ChatGPT.txt']:\n",
        "  f = open(filename, \"r\")\n",
        "\n",
        "  line = f.readline()\n",
        "  while line:\n",
        "    final_dataset['text'].append(line)\n",
        "    final_dataset['method'].append(filename)\n",
        "    line = f.readline()\n",
        "  f.close()\n",
        "final_dataset = pd.DataFrame(final_dataset)\n",
        "extract_features(final_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rri02MVVdUlR",
        "outputId": "f80b7600-4e65-4668-c433-700dc71aa05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 420/420 [00:30<00:00, 13.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols = ['GPT2+ChtGPT.txt', 'GPT2-WP+ChatGPT.txt', 'last_hope_GAN_ChatGPT.txt']\n",
        "feature_cols = ['question_frequency', 'words_per_sentence', 'sttr', 'deixis_frequency', 'tense_present_frequency', 'tense_past_frequency', 'tense_future_frequency', 'person_1_frequency', 'person_2_frequency', 'person_3_frequency', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE', 'ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
        "pos_cols = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB']\n",
        "dep_cols = ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
        "\n",
        "stat_dict = {'dataset' : []}\n",
        "for feat_col in feature_cols:\n",
        "  stat_dict[feat_col + '_mean'] = []\n",
        "  stat_dict[feat_col + '_median'] = []\n",
        "  stat_dict[feat_col + '_std'] = []\n",
        "for col in data_cols:\n",
        "  stat_dict['dataset'].append(col)\n",
        "  for feat_col in feature_cols:\n",
        "    stat_dict[feat_col + '_mean'].append(final_dataset.loc[final_dataset['method'] == col][feat_col].mean())\n",
        "    stat_dict[feat_col + '_median'].append(final_dataset.loc[final_dataset['method'] == col][feat_col].median())\n",
        "    stat_dict[feat_col + '_std'].append(final_dataset.loc[final_dataset['method'] == col][feat_col].std())"
      ],
      "metadata": {
        "id": "ahku0DOgeFoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([wp_dataset, unique_chatgpt_dataset, chatgpt_dataset, comb_dataset]).reset_index(drop=True).to_csv(\"Report3_Outputs.csv\")"
      ],
      "metadata": {
        "id": "G9qgFSzdPsxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"wp_target-Copy.txt\", \"r\")\n",
        "chatgpt_dataset_dict = {'text': []}\n",
        "\n",
        "line = f.readline()\n",
        "c = 0\n",
        "while line:\n",
        "  c += 1\n",
        "  if c == 1000:\n",
        "    break\n",
        "  chatgpt_dataset_dict['text'].append(line)\n",
        "  line = f.readline()\n",
        "\n",
        "chatgpt_dataset = pd.DataFrame(chatgpt_dataset_dict)\n",
        "extract_features(chatgpt_dataset)\n",
        "chatgpt_dataset.to_csv('data_chatgpt.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zKjddOORdjo",
        "outputId": "3af2c781-710b-4b4f-e2c8-f9c8d2dcff61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 999/999 [01:55<00:00,  8.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Features for All Model Outputs"
      ],
      "metadata": {
        "id": "1e73q3r-WEMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = 'drive/MyDrive/OutputsVariousModels/'\n",
        "directory_files = os.listdir(directory_path)\n",
        "\n",
        "for file in directory_files:\n",
        "  if 'last_hope_GAN' not in file and 'nextlevelbaseline' not in file and 'GAN_baseline' not in file:\n",
        "    continue\n",
        "  if '.csv' in file:\n",
        "    continue\n",
        "  f = open(directory_path + file, \"r\")\n",
        "  line_dict = {'text': None}\n",
        "  line_dict['text'] = f.readlines()\n",
        "  line_df = pd.DataFrame(line_dict)\n",
        "  extract_features(line_df)\n",
        "  line_df.to_csv(directory_path + file[:-4] + '.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfEDT1r5HWrA",
        "outputId": "aed7cceb-35c8-4ce3-f0be-882bba3dffaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:25<00:00,  7.94it/s]\n",
            "100%|██████████| 200/200 [00:12<00:00, 16.46it/s]\n",
            "100%|██████████| 200/200 [00:10<00:00, 18.57it/s]\n",
            "100%|██████████| 200/200 [00:09<00:00, 20.60it/s]\n",
            "100%|██████████| 200/200 [00:13<00:00, 14.39it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.80it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.86it/s]\n",
            "100%|██████████| 200/200 [00:10<00:00, 18.45it/s]\n",
            "100%|██████████| 200/200 [00:11<00:00, 16.67it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 13.32it/s]\n",
            "100%|██████████| 200/200 [00:11<00:00, 17.59it/s]\n",
            "100%|██████████| 200/200 [00:11<00:00, 17.94it/s]\n",
            "100%|██████████| 200/200 [00:10<00:00, 19.45it/s]\n",
            "100%|██████████| 200/200 [00:10<00:00, 19.16it/s]\n",
            "100%|██████████| 200/200 [00:13<00:00, 15.23it/s]\n",
            "100%|██████████| 200/200 [00:11<00:00, 18.18it/s]\n",
            "100%|██████████| 200/200 [00:08<00:00, 22.28it/s]\n",
            "100%|██████████| 179/179 [00:07<00:00, 23.81it/s]\n",
            "100%|██████████| 179/179 [00:09<00:00, 18.18it/s]\n",
            "100%|██████████| 179/179 [00:07<00:00, 22.76it/s]\n",
            "100%|██████████| 47/47 [00:03<00:00, 15.55it/s]\n",
            "100%|██████████| 200/200 [00:10<00:00, 19.90it/s]\n",
            "100%|██████████| 200/200 [00:10<00:00, 18.63it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 13.33it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.47it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 14.00it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.93it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 13.32it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.57it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.59it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.91it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.91it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 11.91it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 14.04it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.50it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.46it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.74it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.75it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.77it/s]\n",
            "100%|██████████| 200/200 [00:14<00:00, 13.37it/s]\n",
            "100%|██████████| 200/200 [00:13<00:00, 14.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get phrase repetitions"
      ],
      "metadata": {
        "id": "sqsLGM_ZiMCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "WrKje0cPicAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"wp_target-Copy.txt\", \"r\")\n",
        "max_phrase_length = 7\n",
        "phrases = {}\n",
        "\n",
        "line = f.readline()\n",
        "while line:\n",
        "  phrase_set = set()\n",
        "  doc = nlp(line)\n",
        "  tokens = [token.text.lower() for token in doc]\n",
        "  for k in range(1, max_phrase_length+1):\n",
        "    for j in range(len(tokens) - k):\n",
        "      phrase_set.add(tuple(tokens[j:j+k]))\n",
        "\n",
        "  for p in phrase_set:\n",
        "    if p in phrases:\n",
        "      phrases[p] += 1\n",
        "    else:\n",
        "      phrases[p] = 1\n",
        "\n",
        "  line = f.readline()"
      ],
      "metadata": {
        "id": "m658GebXihz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_phrases = sorted(phrases.items(), key=lambda x:x[1], reverse=True)"
      ],
      "metadata": {
        "id": "x0Ei3WPQksnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for phrase in sorted_phrases:\n",
        "  if len(phrase[0]) > 3 and phrase[1] > 450:\n",
        "    print(phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDRc79v2qSPv",
        "outputId": "a9a13b16-010d-4f05-8ca3-afc97462d4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('once', 'upon', 'a', 'time'), 4045)\n",
            "(('upon', 'a', 'time', ','), 3884)\n",
            "(('once', 'upon', 'a', 'time', ','), 3883)\n",
            "(('>', 'one', 'day', ','), 2985)\n",
            "(('in', 'the', 'end', ','), 2956)\n",
            "((',', 'there', 'was', 'a'), 2952)\n",
            "(('.', 'it', 'was', 'a'), 2775)\n",
            "(('a', 'time', ',', 'there'), 2289)\n",
            "(('upon', 'a', 'time', ',', 'there'), 2288)\n",
            "(('once', 'upon', 'a', 'time', ',', 'there'), 2287)\n",
            "(('time', ',', 'there', 'was'), 2180)\n",
            "(('>', 'at', 'first', ','), 2174)\n",
            "(('a', 'time', ',', 'there', 'was'), 2160)\n",
            "(('upon', 'a', 'time', ',', 'there', 'was'), 2159)\n",
            "(('once', 'upon', 'a', 'time', ',', 'there', 'was'), 2158)\n",
            "(('>', 'in', 'the', 'end', ','), 2156)\n",
            "(('>', 'in', 'the', 'end'), 2156)\n",
            "(('time', ',', 'there', 'was', 'a'), 2067)\n",
            "(('a', 'time', ',', 'there', 'was', 'a'), 2059)\n",
            "(('upon', 'a', 'time', ',', 'there', 'was', 'a'), 2058)\n",
            "(('could', \"n't\", 'help', 'but'), 1822)\n",
            "(('>', 'from', 'that', 'day'), 1522)\n",
            "(('from', 'that', 'day', 'on'), 1312)\n",
            "(('from', 'that', 'day', 'on', ','), 1309)\n",
            "(('that', 'day', 'on', ','), 1309)\n",
            "((',', 'but', 'it', 'was'), 1276)\n",
            "(('once', 'upon', 'a', 'time', ',', 'in'), 1075)\n",
            "(('a', 'time', ',', 'in'), 1075)\n",
            "(('upon', 'a', 'time', ',', 'in'), 1075)\n",
            "(('.', '<', 'newline><newline', '>'), 1064)\n",
            "(('>', 'and', 'so', ','), 1055)\n",
            "((\"n't\", 'help', 'but', 'feel'), 1048)\n",
            "(('could', \"n't\", 'help', 'but', 'feel'), 1008)\n",
            "(('>', 'from', 'that', 'day', 'on'), 977)\n",
            "(('>', 'from', 'that', 'day', 'on', ','), 975)\n",
            "((',', 'i', 'could', \"n't\"), 937)\n",
            "(('.', 'at', 'first', ','), 937)\n",
            "(('time', ',', 'in', 'a'), 935)\n",
            "(('upon', 'a', 'time', ',', 'in', 'a'), 935)\n",
            "(('once', 'upon', 'a', 'time', ',', 'in', 'a'), 935)\n",
            "(('a', 'time', ',', 'in', 'a'), 935)\n",
            "(('i', 'could', \"n't\", 'help'), 861)\n",
            "(('felt', 'a', 'sense', 'of'), 850)\n",
            "(('i', 'could', \"n't\", 'help', 'but'), 831)\n",
            "((',', 'and', 'it', 'was'), 828)\n",
            "(('one', 'day', ',', 'a'), 798)\n",
            "(('.', 'he', 'knew', 'that'), 772)\n",
            "(('he', 'knew', 'that', 'he'), 758)\n",
            "(('as', 'time', 'went', 'on'), 738)\n",
            "(('.', '\"<newline><newline', '>', 'the'), 734)\n",
            "(('as', 'time', 'went', 'on', ','), 726)\n",
            "(('time', 'went', 'on', ','), 726)\n",
            "(('.', 'one', 'day', ','), 725)\n",
            "((',', 'i', 'realized', 'that'), 725)\n",
            "(('.', 'it', 'was', \"n't\"), 714)\n",
            "(('it', 'was', 'as', 'if'), 697)\n",
            "((',', '\"', 'he', 'said'), 694)\n",
            "((',', 'and', 'he', 'was'), 681)\n",
            "(('the', 'end', ',', 'the'), 680)\n",
            "(('in', 'the', 'end', ',', 'the'), 679)\n",
            "(('i', 'knew', 'that', 'i'), 675)\n",
            "(('.', 'it', 'was', 'as'), 673)\n",
            "(('>', 'it', 'was', 'a'), 672)\n",
            "((',', 'i', 'could', \"n't\", 'help'), 658)\n",
            "(('there', 'was', 'a', 'young'), 650)\n",
            "((',', 'i', 'knew', 'that'), 650)\n",
            "((',', 'i', 'could', \"n't\", 'help', 'but'), 647)\n",
            "(('>', 'it', 'was', \"n't\"), 638)\n",
            "(('.', '\"<newline><newline', '>', 'i'), 633)\n",
            "(('was', 'a', 'man', 'named'), 630)\n",
            "(('from', 'that', 'day', 'forward'), 625)\n",
            "(('there', 'was', 'a', 'man'), 625)\n",
            "(('that', 'day', 'forward', ','), 622)\n",
            "(('from', 'that', 'day', 'forward', ','), 622)\n",
            "(('feel', 'a', 'sense', 'of'), 621)\n",
            "(('one', 'day', ',', 'while'), 621)\n",
            "(('at', 'first', ',', 'i'), 608)\n",
            "(('knew', 'that', 'he', 'had'), 608)\n",
            "(('.', 'it', 'was', 'as', 'if'), 607)\n",
            "(('help', 'but', 'feel', 'a'), 599)\n",
            "(('but', 'one', 'day', ','), 599)\n",
            "((',', 'there', 'was', 'a', 'young'), 599)\n",
            "(('for', 'the', 'first', 'time'), 587)\n",
            "((\"n't\", 'help', 'but', 'feel', 'a'), 583)\n",
            "(('there', 'was', 'a', 'man', 'named'), 580)\n",
            "(('years', 'went', 'by', ','), 578)\n",
            "(('>', 'one', 'day', ',', 'a'), 569)\n",
            "(('as', 'time', 'passed', ','), 568)\n",
            "(('for', 'a', 'moment', ','), 562)\n",
            "(('could', \"n't\", 'help', 'but', 'feel', 'a'), 561)\n",
            "((',', 'and', 'i', 'was'), 558)\n",
            "(('>', 'in', 'the', 'end', ',', 'the'), 553)\n",
            "(('did', \"n't\", 'know', 'what'), 551)\n",
            "(('.', 'it', 'was', 'like'), 533)\n",
            "(('it', 'was', \"n't\", 'until'), 532)\n",
            "((',', 'one', 'day', ','), 530)\n",
            "((',', 'and', 'they', 'were'), 528)\n",
            "(('in', 'the', 'middle', 'of'), 524)\n",
            "(('days', 'turned', 'into', 'weeks'), 522)\n",
            "(('in', 'the', 'world', '.'), 517)\n",
            "((',', 'a', 'group', 'of'), 516)\n",
            "((',', 'there', 'was', 'a', 'man'), 515)\n",
            "(('.', 'from', 'that', 'day'), 513)\n",
            "(('i', 'could', \"n't\", 'help', 'but', 'feel'), 509)\n",
            "((',', 'i', 'felt', 'a'), 509)\n",
            "(('.', 'i', 'could', \"n't\"), 508)\n",
            "(('did', \"n't\", 'want', 'to'), 502)\n",
            "(('time', ',', 'there', 'was', 'a', 'young'), 501)\n",
            "(('a', 'time', ',', 'there', 'was', 'a', 'young'), 500)\n",
            "(('took', 'a', 'deep', 'breath'), 498)\n",
            "(('could', \"n't\", 'shake', 'the'), 494)\n",
            "(('.', 'he', 'had', 'been'), 493)\n",
            "((',', 'there', 'was', 'a', 'man', 'named'), 491)\n",
            "(('.', 'he', 'realized', 'that'), 490)\n",
            "(('find', 'a', 'way', 'to'), 479)\n",
            "(('.', 'but', 'as', 'the'), 477)\n",
            "(('realized', 'that', 'he', 'had'), 468)\n",
            "(('began', 'to', 'realize', 'that'), 466)\n",
            "(('time', ',', 'there', 'was', 'a', 'man'), 464)\n",
            "(('a', 'time', ',', 'there', 'was', 'a', 'man'), 464)\n",
            "((',', 'and', 'i', 'could'), 460)\n",
            "(('.', 'they', 'knew', 'that'), 459)\n",
            "((',', 'he', 'realized', 'that'), 458)\n",
            "(('it', 'was', 'just', 'a'), 454)\n",
            "((\"n't\", 'shake', 'the', 'feeling'), 454)\n",
            "(('slowly', 'but', 'surely', ','), 452)\n",
            "(('>', 'over', 'time', ','), 451)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_phrases[:9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0_TgDaLqNll",
        "outputId": "3e2c0a39-713d-4dc5-e0a4-55ec375217fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',',), 14348),\n",
              " (('.',), 14348),\n",
              " (('the',), 14343),\n",
              " (('to',), 14340),\n",
              " (('and',), 14340),\n",
              " (('a',), 14338),\n",
              " (('>',), 14309),\n",
              " (('of',), 14279),\n",
              " (('in',), 14213)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5xlk6-7sWXqx",
        "VACTw33VxVKL",
        "lPYhYrNcI7mf",
        "3mpLFu-X5eOv",
        "GUJxHfsE8HmX",
        "FM4YITS70ivZ",
        "tTg7COsGrFQu",
        "uFOY6OrpuM5F",
        "vIHe1qkNvGF3",
        "0dT5x4qcQIGw",
        "5sjESA0RWPoi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}